^T: Web Scraping and RESTful APIs
^ST: Scraping and APIs
^I: Introduction to Programming for Public Policy
^SI: Intro Programming
^D: November 2, 2016
^H: \include{pythonlst}

+ A Spectrum
\Large{
- (No data or senseless tabulations.)
- Unformatted data and locked down websites...
- Tables on nice, clean webpages.
- Slightly-hidden APIs, needing a bit of scraping.
- Documented APIs, ready for consumption.
}

+ Examples of Different Resources
Qualities and techniques best demonstrated by example:
# Google has many straightforward APIs for \link{https://developers.google.com/maps/documentation/directions/intro\#traffic-model}{mapping}.
# Twitter famously provides one of the most-comfortable, featureful, compulsively perfect \link{https://developer.twitter.com/en/docs/api-reference-index}{APIs} ever (\link{https://github.com/harris-ippp/lectures/blob/master/06-web/ex/twitter\_stream.py}{example}).
# Census provides both a `consumer-level' \link{http://www.census.gov/data.html}{site} and \link{http://www.census.gov/data/developers/data-sets/acs-5year.html}{an API} for retrieving well-formatted data at any level.
# Bureau of Labor Statistics provides a \link{http://www.bls.gov/developers/api_python.htm}{respectable API} for time series data whose \link{http://www.bls.gov/help/hlpforma.htm}{coding} is quite abstruse (\link{https://github.com/harris-ippp/lectures/blob/master/06-web/ex/bls_time_series.py}{example}).
# The \link{http://historical.elections.virginia.gov/elections/search/year_from:1924/year_to:2016/office_id:1/stage:General}{Virginia Department of Elections} provides a clean (but hidden) interface for retrieving data, but no clean way to {\it query it}.
# Lots of Wikipedia \link{https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_the_United_States_by_endowment}{articles} have ``pretty clean" tables.
# Illinois State Board of Education \link{http://illinoisreportcard.com/}{Report Card} provides a lot of data, but back-breaking methods for accessing it.
# Some websites go to great lengths to keep you out: \link{http://indiawater.gov.in/IMISReports/Reports/WaterQuality/rpt_WQM_HabitationWiseLabTesting_S.aspx?Rep=0\&RP=Y}{India Water} or \link{https://trends.google.com/trends/}{google trends} [\link{https://trends.google.com/trends/api/explore?hl=en-US\&property=\&req=\%7B\%22comparisonItem\%22\%3A+\%5B\%7B\%22geo\%22\%3A+\%22\%22\%2C+\%22keyword\%22\%3A+\%22apples\%22\%2C+\%22time\%22\%3A+\%22today+5-y\%22\%7D\%2C+\%7B\%22geo\%22\%3A+\%22\%22\%2C+\%22keyword\%22\%3A+\%22bananas\%22\%2C+\%22time\%22\%3A+\%22today+5-y\%22\%7D\%5D\%2C+\%22category\%22\%3A+0\%7D\&tz=360}{ex}].  (It is {\it their data}, after all...)

+ Five Tools
# \link{http://docs.python-requests.org/en/master/}{requests} library: retrieve web resources \emph{in python}.
#- Provides methods for authentication, \tt{POST}ing data, etc.
#- Basically, \tt{curl} for python.
# \link{http://selenium-python.readthedocs.io/}{selenium} has similar functionality, but completes javascript loads.
# \link{https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read\_html.html}{pandas.read\_html}: for well-formatted tables, pandas does great.
# \link{https://www.crummy.com/software/BeautifulSoup/bs4/doc/}{beautifulsoup} library: provides mechanisms for `quickly' accessing and extracting elements of web pages in python.
# curl -- our old friend!

+++ Scraping

+ What is Scraping?
- Look at the actual html and the individual requests, using the developpers tools in your browser.
- \chred{requests}: Identify patterns in HTML and URLs that allow you to download the appropriate resources.
- \chred{BeautifulSoup}: If necessary, extract data \emph{from} those resources.
- Necessarily a one-off for each site, and often for each part of a site.

+ Downloading Data: Requests
- Download data directly -- for example, our \link{https://harris-ippp.github.io/lectures/}{chickens} page.
\pythonexternal{ex/req.py}

+ Extracting Data: Beautiful Soup 4
Beauiful Soup: full documentation \link{https://www.crummy.com/software/BeautifulSoup/bs4/doc/}{here}.
- If you can identify the objects, \tt{find()} and \tt{find\_all()} are usually the fastest accessors.
-- These yield the first, and \emph{all} instances, respectively.
- Consider this example, from our website:
\pythonexternal{ex/bs_find.py}
- Find all the rows in the table:
` soup.find\_all("tr")

+ BS4: Parts of Elements
Accessing parts of elements get/dictionary:
- \tt{soup.find("img").get("src")}
- \tt{soup.find("ul").get("class")}
- \tt{soup.find("em").contents}
- \tt{soup.find("ul")["class"]}
- \tt{soup.find("a")["href"]}

+ BS4: Children and Contents
- Select all of the `true' conditions of chicken health benefits.
- The problem is that the truth is in a different element from the item.
- We need to to look at rows in their entirety, printing the first column if the second is true.
- \tt{children()} provides an iterable, and \tt{.contents} provides a list.
>1em
\link{https://raw.githubusercontent.com/harris-ippp/lectures/master/06-web/ex/find\_true.py}{Solution}

+ For ``Trivial" Tables: Pandas
Pandas will extract tables from a page, as a list of DataFrames.
>1em
\pythonexternal{ex/panda_scrape.py}

+ Beautiful Soup
- For more-difficult sites, there is an `art' to scanning the raw html/resources and finding the tag you want.
- Take a \link{https://illinoisreportcard.com/School.aspx?schoolid=150162990250849}{school} from the \link{http://illinoisreportcard.com/District.aspx?source=schoolsindistrict&Districtid=15016299025}{Illinois State Report Card} (which we used last week): 560 schools in the district, not doing it by hand.
-- Check the html to see if you can find the address?  No!?
-- Chrome: View $→$ Developper $→$ Developper Tools $→$ Elements.
-- Firefox: Tools $→$ Developper $→$ Toggle Tools $→$ Inspector.
-- Find the element of interest... find \emph{its} source.
- Most complex webpages load content from many different sources; they may not all be rendered as part of the base URL.
-- If watching for multiple components, watch ``Network," also in Developper Tools.
- Best case: find it in the webpage or wait for it to load.

+ Beautiful Soup: ``Real" Example
- Please grab the url from this \link{https://illinoisreportcard.com/School.aspx?schoolid=150162990250849}{link}.
- Go to View/Tools $→$ Source, and look for the address.
\pythonexternal{ex/bs.py}

+ Scraping with a WebDriver: Selenium
- On \link{http://illinoisreportcard.com/District.aspx?source=SchoolsInDistrict&Districtid=15016299025}{many} web pages, you must wait for elements of a page to load from other sources.  These won't be in the html.
- Browsers aren't scalable, so use a python a web driver!
` \pr conda install -c conda-forge selenium \\ \pr conda install phantomjs
\pythonexternal{ex/sel.py}

\centering 
Reducing the Pain:
+++ RESTful APIs
\centering 
Representational State Transfer: Doing it the `Right' Way.

+ REST in Practice
- Application Programming Interfaces (APIs) are exposed resources with documented parameters for returning formatted data.
- Usually: \tt{GET} requests that return json, csv, or xml data.
-- Variables follow a ?, are specified by =, and separated by \&.
! APIs: Fancy-Coded URLs
- You can then access these resources with \tt{requests}, \tt{curl}, etc.

+ RESTful Principles\footnote{The most-readable resource on this I have found is \link{http://www.drdobbs.com/web-development/restful-web-services-a-tutorial/240169069?pgno=1}{here}.}
Roy Fielding standardized good principles for HTTP 1.1 and RESTful services.
REST is a consistent \emph{style} of organizing resources. 
The philosophy is that:
- Client and server are stateless and separated (server doesn't `remember' anything about a session).
- Service is scalable and cachable; this allows for expansion.
- RESTful web services (usually) use HTTP methods as meaningful verbs, and web addresses as functions.
-- \tt{GET} is a pure retrieval and \tt{POST} corresponds to a send.  
-- \tt{DELETE}, \tt{PUT}, etc. may also be defined.

+ Census API
- Typical, excellent, APIs from the \link{http://www.census.gov/data/developers/data-sets.html}{Census}.
- For example, five-year \link{http://api.census.gov/data/2014/acs5/examples.html}{ACS estimates} as of 2014 (\link{http://api.census.gov/data/2014/acs5/profile/variables.html}{variables}).
\centering
\fontsize{8.5}{12} \selectfont
\link{http://api.census.gov/data/2014/acs5/profile?get=DP02\_0037PE,NAME\&for=state:*}{http://api.census.gov/data/2014/acs5/profile?get=DP02\_0037PE,NAME\&for=state:*}
\normalsize
<1em
- Returning to last week:
\pythonexternal{ex/census.py}

+ Google Maps API
Well-documented APIs for \link{https://developers.google.com/maps/documentation/geocoding/start}{geocoding}, \link{https://developers.google.com/maps/documentation/directions/start}{directions}, \link{https://developers.google.com/maps/documentation/distance-matrix/intro}{distances}, etc.
- Returns a beautiful json response.
- For more than a few calls, requires a (free) API key.
\pythonexternal{ex/google.py}

+ Unexposed APIs
Some websites establish pretty good RESTful interfaces, and make no effort to restrict their use -- but don't publicize them either.
- Let's try \link{https://vizhub.healthdata.org/gbd-compare/}{healthdata.org}.
- Watch the Network as we navigate the visualization, filter for gbd-compare/api.
- Just need the metadata, which shows up at the \link{https://vizhub.healthdata.org/gbd-compare/api/metadata?lang=41}{beginning}.
>1em
Some websites make you work a little bit harder:
- Google \link{https://trends.google.com/trends/}{trends} requires a token, but it's fairly easy to find it. [\link{https://trends.google.com/trends/api/explore?hl=en-US\&property=\&req=\%7B\%22comparisonItem\%22\%3A+\%5B\%7B\%22geo\%22\%3A+\%22\%22\%2C+\%22keyword\%22\%3A+\%22apples\%22\%2C+\%22time\%22\%3A+\%22today+5-y\%22\%7D\%2C+\%7B\%22geo\%22\%3A+\%22\%22\%2C+\%22keyword\%22\%3A+\%22bananas\%22\%2C+\%22time\%22\%3A+\%22today+5-y\%22\%7D\%5D\%2C+\%22category\%22\%3A+0\%7D\&tz=360}{ex}]
>1em
Some sites make it fairly hard to retrieve their data (changing tokens/authentication).

+ Scraping Take-Aways
- Enormous variability in ease of scraping and tools required.
- Hope for good APIs: immediate retrieval with requests.
- Well-formatted tables also fine: pandas saves the day.
- ``Deep" scraping takes BeautifulSoup, practice, and patience.

\backup
+++ Notes and Rants

+ Twitter API
- Fantastic, clear API: \link{https://api.twitter.com/1.1/}{https://api.twitter.com/1.1/}
- \link{https://dev.twitter.com/rest/reference}{Many methods}, e.g., show users or tweets by user:
` /users/show.json?screen\_name=iamjohnoliver \\ /statuses/user\_timeline.json?screen\_name=iamjohnoliver
- Basically: carefully follow each model.  Query starts by ? and terms are separated by \& (except in \tt{search}).
- Requires access keys, readily generated with a Twitter account.\footnote{Follow \link{https://github.com/harris-ippp/lectures/blob/master/06-web/twitter.md}{these instructions} and checkout \link{https://github.com/harris-ippp/lectures/blob/master/06-web/ex/twitter_query.py}{this script} if you want to try this.}

+ Two Notes on APIs
# A lot of APIs end up with 3rd party python libaries. 
#- For two examples, these are tweepy and sunlightlabs/census.
#- Both of them are good!  But they obfuscate the original intention, and often are not as well documented as the original site.
#- \emph{I} usually find it easier just to understand the API. \\ \vspace{3em}
# Many cities/states use Socrata or CKAN (open source/data.gov).  Socrata comes with the SODA API, for many (all?) datasets (e.g., \link{https://dev.socrata.com/foundry/data.cityofchicago.org/fg6s-gzvg}{Chicago Divvy}).

+ A Soapbox on Standards
- Tremendous range in \emph{how hard} you have to work to get data.
- Lots of jurisdictions and agencies are touting their open data efforts.  But very often, the released data sets are awful.
- Even when they're very good (e.g., city crime, education) they may not be standardized across jurisdictions.  
- Need standards (schemas) to minimize overlapping efforts.

